# Visual-Question-Answering-VQA-on-Vizwiz-dataset
 We have worked on the Vizwiz dataset ( https://vizwiz.org/tasks-and-datasets/vqa/ ) for the visual question answering purpose using OpenAI Clip base patch32 ( https://huggingface.co/openai/clip-vit-base-patch32 ). We have created two types of answer vocabulary (we can understand from the code) using both the train and validation's answers and since the second approach giving us more answer list that's why we finzlise the second vocabulary with 6546 unique words (Although both vocabulary are giving the same results). We have extracted features of question and images using the above model and using those extracted features, we have applied on Linear layer. But the Linear layer shows overfitting that's why we choose a two layer MLP with ReLU activationa and a dropout of 0.5, after epoch 7 we saved our model with 71% train accuracy and 64% validation accuracy.
